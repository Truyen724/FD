{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import datetime\n",
    "import time \n",
    "from mtcnn.detector import MtcnnDetector\n",
    "import cv2\n",
    "from align_faces import warp_and_crop_face, get_reference_facial_points\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = MtcnnDetector()\n",
    "def face_detection(img, output_size, align = True):\n",
    "    \"\"\"Nếu align == True thì sẽ có thêm bước Face Alignment sau khi nhận diện gương mặt,\n",
    "        ngược lại thì không có thêm bước này\"\"\"\n",
    "    \n",
    "    # Nhận diện gương mặt\n",
    "    boxes, facial5points = detector.detect_faces(img)\n",
    "    \n",
    "    # Tiến hành Alignment\n",
    "    if align == True:\n",
    "        if len(facial5points) != 0:\n",
    "            facial5points = np.reshape(facial5points[0], (2, 5))\n",
    "\n",
    "            default_square = True\n",
    "            inner_padding_factor = 0.25\n",
    "            outer_padding = (0, 0)\n",
    "\n",
    "            ## Yêu cầu 10:\n",
    "            ## VIẾT CODE Ở ĐÂY:\n",
    "\n",
    "            # sử dụng hàm get_reference_facial_points\n",
    "            reference_5pts = get_reference_facial_points(\n",
    "                output_size, inner_padding_factor, outer_padding, default_square)\n",
    "            \n",
    "            # sử dụng hàm warp_and_crop_face\n",
    "            face = warp_and_crop_face(img, facial5points, reference_pts = reference_5pts, crop_size = output_size)\n",
    "\n",
    "    # Không Alignment\n",
    "    else:\n",
    "        (h,w) = img.shape[:2]\n",
    "        if len(boxes) != 0:\n",
    "            for box in boxes:\n",
    "                (startX, startY, endX, endY) = box[:4].astype('int')\n",
    "                (startX, startY) = (max(0, startX),max(0, startY))\n",
    "                (endX, endY) = (min(w-1, endX), min(h-1, endY))\n",
    "                face = img[startY:endY, startX:endX]\n",
    "                face = cv2.resize(face, output_size)\n",
    "                \n",
    "    return face\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect(image):\n",
    "    \n",
    "    img = image.copy()\n",
    "    (h,w) = img.shape[:2]\n",
    "    boxes, facial5points = detector.detect_faces(img)\n",
    "    for box in boxes:\n",
    "        (startX,startY,endX,endY)=box[:4].astype('int')\n",
    "\n",
    "        #ensure the bounding boxes fall within the dimensions of the frame\n",
    "        (startX,startY)=(max(0,startX),max(0,startY))\n",
    "        (endX,endY)=(min(w-1,endX), min(h-1,endY))\n",
    "\n",
    "        #extract the face ROI, convert it from BGR to RGB channel, resize it to 224,224 and preprocess it\n",
    "        face=img[startY:endY, startX:endX]\n",
    "        face=cv2.resize(face,(160,160))\n",
    "\n",
    "        # (mask,withoutMask) = model.predict(face.reshape(1,224,224,3))[0]\n",
    "\n",
    "        # #determine the class label and color we will use to draw the bounding box and text\n",
    "        # label='Mask' if mask>withoutMask else 'No Mask'\n",
    "        # color=(0,255,0) if label=='Mask' else (0,0,255)\n",
    "        # color=(0,255,0)\n",
    "        #include the probability in the label\n",
    "        # label=\"{}: {:.2f}%\".format(label,max(mask,withoutMask)*100)\n",
    "        # label = \"Truyen\"\n",
    "        #display the label and bounding boxes\n",
    "        # cv2.putText(img,label,(startX,startY-10),cv2.FONT_HERSHEY_SIMPLEX,0.45,color,2)\n",
    "        # cv2.rectangle(img,(startX,startY),(endX,endY),color,2)\n",
    "        \n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = InceptionResnetV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch \n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "IMG_PATH = \"Data/Image\"\n",
    "DATA_PATH = './data'\n",
    "embeddings = []\n",
    "names = []\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(img):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            fixed_image_standardization\n",
    "        ])\n",
    "    return transform(img).unsqueeze(0)\n",
    "def fixed_image_standardization(image_tensor):\n",
    "    processed_tensor = (image_tensor - 127.5) / 128.0\n",
    "    return processed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionResnetV1(\n",
    "    classify=False,\n",
    "    pretrained=\"casia-webface\"\n",
    ").to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    }
   ],
   "source": [
    "for usr in os.listdir(IMG_PATH):\n",
    "    embeds = []\n",
    "    for file in glob.glob(os.path.join(IMG_PATH, usr)+'/*.jpg'):\n",
    "        try:\n",
    "            # img = Image.open(file)\n",
    "            img = cv2.imread(file)\n",
    "            face = face_detect(img)\n",
    "            cv2.imshow(\"face\", face)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        except:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            embed = model(trans(face).to(device))\n",
    "            embeds.append(embed) #1 anh, kich thuoc [1,512]\n",
    "    if len(embeds) == 0:\n",
    "        continue\n",
    "    embedding = torch.cat(embeds).mean(0, keepdim=True) #dua ra trung binh cua 50 anh, kich thuoc [1,512]\n",
    "    embeddings.append(embedding.numpy()) # 1 cai list n cai [1,512]\n",
    "    names.append(usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nam', 'Ngan', 'Truyen', 'Unknow1', 'Unknow2', 'Unknow3']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(names)\n",
    "print(len(embeds))\n",
    "# print(np.sqrt(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_face(img):\n",
    "    with torch.no_grad():\n",
    "        embed = model(trans(img).to(device))\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = cv2.imread(\"tocvang.JPG\")\n",
    "# img = cv2.resize(img,(480,720))\n",
    "# img = cv2.resize(img,(128,128))\n",
    "# out_putsize = (128,128)\n",
    "face = face_detect(img)\n",
    "eb_face = embed_face(face)\n",
    "cv2.imshow(\"face\",face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eb_face.shape)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 'Truyen', [0.9999999, 0.9999995, 1.0, 0.9999994, 0.9999994, 0.9999994])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cosin(x, y):\n",
    "    x = x.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "    cos_sim = np.dot(y, x)/(norm(y)*norm(x))\n",
    "    return cos_sim\n",
    "\n",
    "y = eb_face \n",
    "def get_face(face_embed, names, embeddings):\n",
    "    f = lambda x: get_cosin(x, y)\n",
    "    list_cosin = [get_cosin(e1, face_embed) for e1 in embeddings]\n",
    "    list_cosin2 = [cosine_similarity(face_embed, face_embed) for ex in embeddings]\n",
    "    index_of_face = list_cosin.index(max(list_cosin))\n",
    "    name = names[index_of_face]\n",
    "    print(list_cosin2)\n",
    "    return index_of_face, name, list_cosin\n",
    "get_face(eb_face, names, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nam', 'Ngan', 'Truyen', 'Unknow1', 'Unknow2', 'Unknow3']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_recorinize(image):\n",
    "    img = image.copy()\n",
    "    (h,w) = img.shape[:2]\n",
    "    boxes, facial5points = detector.detect_faces(img)\n",
    "    for box in boxes:\n",
    "        (startX,startY,endX,endY)=box[:4].astype('int')\n",
    "\n",
    "        #ensure the bounding boxes fall within the dimensions of the frame\n",
    "        (startX,startY)=(max(0,startX),max(0,startY))\n",
    "        (endX,endY)=(min(w-1,endX), min(h-1,endY))\n",
    "\n",
    "        #extract the face ROI, convert it from BGR to RGB channel, resize it to 224,224 and preprocess it\n",
    "        face=img[startY:endY, startX:endX]\n",
    "        face=cv2.resize(face,(160,160))\n",
    "        get_face()\n",
    "        embed = embed_face(face)\n",
    "        # (mask,withoutMask) = model.predict(face.reshape(1,224,224,3))[0]\n",
    "        \n",
    "        # #determine the class label and color we will use to draw the bounding box and text\n",
    "        # label='Mask' if mask>withoutMask else 'No Mask'\n",
    "        # color=(0,255,0) if label=='Mask' else (0,0,255)\n",
    "        color=(0,255,0)\n",
    "        #include the probability in the label\n",
    "        # label=\"{}: {:.2f}%\".format(label,max(mask,withoutMask)*100)\n",
    "        label = \"Truyen\"\n",
    "        index_of_face, name, list_cosin = get_face()\n",
    "        #display the label and bounding boxes\n",
    "        cv2.putText(img,label,(startX,startY-10),cv2.FONT_HERSHEY_SIMPLEX,0.45,color,2)\n",
    "        cv2.rectangle(img,(startX,startY),(endX,endY),color,2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(id)\n",
    "while True:\n",
    "    x = time.time()\n",
    "    ret, frame = video_capture.read()\n",
    "    # img = frame[0:128,0:128]\n",
    "    # print(model.predict(np.array([img])))\n",
    "    img = face_recorinize(frame)\n",
    "    print(time.time() - x)\n",
    "    cv2.imshow('{}'.format(id), img)        \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nam', 'Ngan', 'Truyen', 'Unknow1', 'Unknow2', 'Unknow3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03484886,  0.0777184 ,  0.03608472, ...,  0.05145345,\n",
       "         0.0110453 , -0.12757151],\n",
       "       [-0.03486102,  0.07773694,  0.03610361, ...,  0.05148026,\n",
       "         0.01106738, -0.12755129],\n",
       "       [-0.03483398,  0.0777238 ,  0.03607829, ...,  0.05144887,\n",
       "         0.01102702, -0.12760642],\n",
       "       [-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "         0.01103554, -0.12758632],\n",
       "       [-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "         0.01103554, -0.12758632],\n",
       "       [-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "         0.01103554, -0.12758632]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(names)\n",
    "# print(embeddings)\n",
    "data = np.asarray(embeddings).reshape(len(embeddings),512)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03484886,  0.0777184 ,  0.03608472, ...,  0.05145345,\n",
       "          0.0110453 , -0.12757151]],\n",
       "\n",
       "       [[-0.03486102,  0.07773694,  0.03610361, ...,  0.05148026,\n",
       "          0.01106738, -0.12755129]],\n",
       "\n",
       "       [[-0.03483398,  0.0777238 ,  0.03607829, ...,  0.05144887,\n",
       "          0.01102702, -0.12760642]],\n",
       "\n",
       "       [[-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "          0.01103554, -0.12758632]],\n",
       "\n",
       "       [[-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "          0.01103554, -0.12758632]],\n",
       "\n",
       "       [[-0.03484415,  0.07775436,  0.03613765, ...,  0.0514464 ,\n",
       "          0.01103554, -0.12758632]]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03484886  0.0777184   0.03608472 ...  0.05145345  0.0110453\n",
      "  -0.12757151]\n",
      " [-0.03486102  0.07773694  0.03610361 ...  0.05148026  0.01106738\n",
      "  -0.12755129]\n",
      " [-0.03483398  0.0777238   0.03607829 ...  0.05144887  0.01102702\n",
      "  -0.12760642]\n",
      " [-0.03484415  0.07775436  0.03613765 ...  0.0514464   0.01103554\n",
      "  -0.12758632]\n",
      " [-0.03484415  0.07775436  0.03613765 ...  0.0514464   0.01103554\n",
      "  -0.12758632]\n",
      " [-0.03484415  0.07775436  0.03613765 ...  0.0514464   0.01103554\n",
      "  -0.12758632]]\n",
      "(6, 512)\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(data.shape[0],512)\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "label = np.array(le.fit_transform(names)).reshape(-1, 1)\n",
    "\n",
    "print(label)\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "label = one_hot.fit_transform(label).toarray()\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLPClassifier(hidden_layer_sizes=(64),max_iter=100,verbose = True)\n",
    "model1.fit(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'classes_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4296/1956831995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'classes_'"
     ]
    }
   ],
   "source": [
    "s = le.fit_transform(names)\n",
    "s.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto', probability=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SVC(gamma='auto',probability = True)\n",
    "model2.fit(data, le.fit_transform(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pred = model2.predict_proba(data)\n",
    "print(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(le.fit_transform(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 512)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto', probability=True))])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto',probability = True))\n",
    "clf.fit(data,le.fit_transform(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, -1, -4, -4, -4], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-clf.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf,open(\"Model/stratify.pkl\",\"wb\"))\n",
    "pickle.dump(le.fit_transform(names),open(\"List_user/lst_user.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    }
   ],
   "source": [
    "i_p = \"sontung.jpg\"\n",
    "i = cv2.imread(i_p)\n",
    "\n",
    "# img = cv2.resize(img,(480,720))\n",
    "# img = cv2.resize(img,(128,128))\n",
    "# out_putsize = (128,128)\n",
    "face = face_detect(i)\n",
    "eb_face = embed_face(face)\n",
    "cv2.imshow(\"face\",face)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9008256746882654\n",
      "0.0020012855529785156\n"
     ]
    }
   ],
   "source": [
    "x = time.time()\n",
    "out = 1 - clf.predict_proba(data)\n",
    "print(np.max(out[0]))\n",
    "print(time.time()-x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nam', 'Ngan', 'Truyen', 'Unknow1', 'Unknow2', 'Unknow3', 'Unknow4']\n"
     ]
    }
   ],
   "source": [
    "lst = pickle.load(open(\"List_user/2022_10_01_11-32-55_ListUser.pkl\",\"rb\"))\n",
    "print(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
